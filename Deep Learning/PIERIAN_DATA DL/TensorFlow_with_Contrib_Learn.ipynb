{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Theory:**\n",
        "This code demonstrates how to train and evaluate a Deep Neural Network (DNN) classifier on the Iris dataset using TensorFlow. The steps involved include data loading, preprocessing, model creation, training, and evaluation.\n",
        "\n",
        "**1. Data Loading:**\n",
        "The Iris dataset is loaded using sklearn.datasets.load_iris(). This dataset contains 150 samples of iris flowers with four features: sepal length, sepal width, petal length, and petal width. Each sample is labeled with one of three classes representing different iris species.\n",
        "\n",
        "**2. Data Splitting:**\n",
        "The dataset is split into training and testing sets using train_test_split() from sklearn.model_selection. This ensures that 70% of the data is used for training and 30% for testing.\n",
        "\n",
        "**3. Feature Columns:**\n",
        "TensorFlow feature columns are defined for each of the four features in the dataset. These feature columns help the model understand the input data structure.\n",
        "\n",
        "**4. Input Functions:**\n",
        "Two input functions, train_input_fn and eval_input_fn, are defined. These functions create TensorFlow datasets from the training and testing data, respectively. The datasets are shuffled and batched to ensure efficient training and evaluation.\n",
        "\n",
        "**5. Model Creation:**\n",
        "A DNNClassifier is created using TensorFlow's tf.estimator.DNNClassifier. This classifier has three hidden layers with 10, 20, and 10 units, respectively, and three output classes (one for each iris species).\n",
        "\n",
        "**6. Model Training:**\n",
        "The model is trained using the classifier.train() method, which takes the training input function and the number of training steps as arguments. The training input function provides batches of training data to the model.\n",
        "\n",
        "**7. Model Evaluation:**\n",
        "The model is evaluated on the test set using the classifier.evaluate() method. The evaluation input function provides batches of test data to the model, and the model's accuracy on the test set is printed.\n",
        "\n",
        "**8. Summary:**\n",
        "This code provides a complete workflow for training and evaluating a DNN classifier on a simple dataset using TensorFlow. The key steps involve data preparation, model definition, training, and evaluation, all implemented using TensorFlow's high-level Estimator API."
      ],
      "metadata": {
        "id": "uXHo4YG6roJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training and Evaluating a DNN Classifier on the Iris Dataset Using TensorFlow**"
      ],
      "metadata": {
        "id": "_9Gh6x1qrhf9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TsEanR7KrmE"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()"
      ],
      "metadata": {
        "id": "MaOp4s0bMgOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_bk1QcBMiwg",
        "outputId": "5ff18b57-77e5-4cd2-de40-88bda83f9040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
              "        [4.9, 3. , 1.4, 0.2],\n",
              "        [4.7, 3.2, 1.3, 0.2],\n",
              "        [4.6, 3.1, 1.5, 0.2],\n",
              "        [5. , 3.6, 1.4, 0.2],\n",
              "        [5.4, 3.9, 1.7, 0.4],\n",
              "        [4.6, 3.4, 1.4, 0.3],\n",
              "        [5. , 3.4, 1.5, 0.2],\n",
              "        [4.4, 2.9, 1.4, 0.2],\n",
              "        [4.9, 3.1, 1.5, 0.1],\n",
              "        [5.4, 3.7, 1.5, 0.2],\n",
              "        [4.8, 3.4, 1.6, 0.2],\n",
              "        [4.8, 3. , 1.4, 0.1],\n",
              "        [4.3, 3. , 1.1, 0.1],\n",
              "        [5.8, 4. , 1.2, 0.2],\n",
              "        [5.7, 4.4, 1.5, 0.4],\n",
              "        [5.4, 3.9, 1.3, 0.4],\n",
              "        [5.1, 3.5, 1.4, 0.3],\n",
              "        [5.7, 3.8, 1.7, 0.3],\n",
              "        [5.1, 3.8, 1.5, 0.3],\n",
              "        [5.4, 3.4, 1.7, 0.2],\n",
              "        [5.1, 3.7, 1.5, 0.4],\n",
              "        [4.6, 3.6, 1. , 0.2],\n",
              "        [5.1, 3.3, 1.7, 0.5],\n",
              "        [4.8, 3.4, 1.9, 0.2],\n",
              "        [5. , 3. , 1.6, 0.2],\n",
              "        [5. , 3.4, 1.6, 0.4],\n",
              "        [5.2, 3.5, 1.5, 0.2],\n",
              "        [5.2, 3.4, 1.4, 0.2],\n",
              "        [4.7, 3.2, 1.6, 0.2],\n",
              "        [4.8, 3.1, 1.6, 0.2],\n",
              "        [5.4, 3.4, 1.5, 0.4],\n",
              "        [5.2, 4.1, 1.5, 0.1],\n",
              "        [5.5, 4.2, 1.4, 0.2],\n",
              "        [4.9, 3.1, 1.5, 0.2],\n",
              "        [5. , 3.2, 1.2, 0.2],\n",
              "        [5.5, 3.5, 1.3, 0.2],\n",
              "        [4.9, 3.6, 1.4, 0.1],\n",
              "        [4.4, 3. , 1.3, 0.2],\n",
              "        [5.1, 3.4, 1.5, 0.2],\n",
              "        [5. , 3.5, 1.3, 0.3],\n",
              "        [4.5, 2.3, 1.3, 0.3],\n",
              "        [4.4, 3.2, 1.3, 0.2],\n",
              "        [5. , 3.5, 1.6, 0.6],\n",
              "        [5.1, 3.8, 1.9, 0.4],\n",
              "        [4.8, 3. , 1.4, 0.3],\n",
              "        [5.1, 3.8, 1.6, 0.2],\n",
              "        [4.6, 3.2, 1.4, 0.2],\n",
              "        [5.3, 3.7, 1.5, 0.2],\n",
              "        [5. , 3.3, 1.4, 0.2],\n",
              "        [7. , 3.2, 4.7, 1.4],\n",
              "        [6.4, 3.2, 4.5, 1.5],\n",
              "        [6.9, 3.1, 4.9, 1.5],\n",
              "        [5.5, 2.3, 4. , 1.3],\n",
              "        [6.5, 2.8, 4.6, 1.5],\n",
              "        [5.7, 2.8, 4.5, 1.3],\n",
              "        [6.3, 3.3, 4.7, 1.6],\n",
              "        [4.9, 2.4, 3.3, 1. ],\n",
              "        [6.6, 2.9, 4.6, 1.3],\n",
              "        [5.2, 2.7, 3.9, 1.4],\n",
              "        [5. , 2. , 3.5, 1. ],\n",
              "        [5.9, 3. , 4.2, 1.5],\n",
              "        [6. , 2.2, 4. , 1. ],\n",
              "        [6.1, 2.9, 4.7, 1.4],\n",
              "        [5.6, 2.9, 3.6, 1.3],\n",
              "        [6.7, 3.1, 4.4, 1.4],\n",
              "        [5.6, 3. , 4.5, 1.5],\n",
              "        [5.8, 2.7, 4.1, 1. ],\n",
              "        [6.2, 2.2, 4.5, 1.5],\n",
              "        [5.6, 2.5, 3.9, 1.1],\n",
              "        [5.9, 3.2, 4.8, 1.8],\n",
              "        [6.1, 2.8, 4. , 1.3],\n",
              "        [6.3, 2.5, 4.9, 1.5],\n",
              "        [6.1, 2.8, 4.7, 1.2],\n",
              "        [6.4, 2.9, 4.3, 1.3],\n",
              "        [6.6, 3. , 4.4, 1.4],\n",
              "        [6.8, 2.8, 4.8, 1.4],\n",
              "        [6.7, 3. , 5. , 1.7],\n",
              "        [6. , 2.9, 4.5, 1.5],\n",
              "        [5.7, 2.6, 3.5, 1. ],\n",
              "        [5.5, 2.4, 3.8, 1.1],\n",
              "        [5.5, 2.4, 3.7, 1. ],\n",
              "        [5.8, 2.7, 3.9, 1.2],\n",
              "        [6. , 2.7, 5.1, 1.6],\n",
              "        [5.4, 3. , 4.5, 1.5],\n",
              "        [6. , 3.4, 4.5, 1.6],\n",
              "        [6.7, 3.1, 4.7, 1.5],\n",
              "        [6.3, 2.3, 4.4, 1.3],\n",
              "        [5.6, 3. , 4.1, 1.3],\n",
              "        [5.5, 2.5, 4. , 1.3],\n",
              "        [5.5, 2.6, 4.4, 1.2],\n",
              "        [6.1, 3. , 4.6, 1.4],\n",
              "        [5.8, 2.6, 4. , 1.2],\n",
              "        [5. , 2.3, 3.3, 1. ],\n",
              "        [5.6, 2.7, 4.2, 1.3],\n",
              "        [5.7, 3. , 4.2, 1.2],\n",
              "        [5.7, 2.9, 4.2, 1.3],\n",
              "        [6.2, 2.9, 4.3, 1.3],\n",
              "        [5.1, 2.5, 3. , 1.1],\n",
              "        [5.7, 2.8, 4.1, 1.3],\n",
              "        [6.3, 3.3, 6. , 2.5],\n",
              "        [5.8, 2.7, 5.1, 1.9],\n",
              "        [7.1, 3. , 5.9, 2.1],\n",
              "        [6.3, 2.9, 5.6, 1.8],\n",
              "        [6.5, 3. , 5.8, 2.2],\n",
              "        [7.6, 3. , 6.6, 2.1],\n",
              "        [4.9, 2.5, 4.5, 1.7],\n",
              "        [7.3, 2.9, 6.3, 1.8],\n",
              "        [6.7, 2.5, 5.8, 1.8],\n",
              "        [7.2, 3.6, 6.1, 2.5],\n",
              "        [6.5, 3.2, 5.1, 2. ],\n",
              "        [6.4, 2.7, 5.3, 1.9],\n",
              "        [6.8, 3. , 5.5, 2.1],\n",
              "        [5.7, 2.5, 5. , 2. ],\n",
              "        [5.8, 2.8, 5.1, 2.4],\n",
              "        [6.4, 3.2, 5.3, 2.3],\n",
              "        [6.5, 3. , 5.5, 1.8],\n",
              "        [7.7, 3.8, 6.7, 2.2],\n",
              "        [7.7, 2.6, 6.9, 2.3],\n",
              "        [6. , 2.2, 5. , 1.5],\n",
              "        [6.9, 3.2, 5.7, 2.3],\n",
              "        [5.6, 2.8, 4.9, 2. ],\n",
              "        [7.7, 2.8, 6.7, 2. ],\n",
              "        [6.3, 2.7, 4.9, 1.8],\n",
              "        [6.7, 3.3, 5.7, 2.1],\n",
              "        [7.2, 3.2, 6. , 1.8],\n",
              "        [6.2, 2.8, 4.8, 1.8],\n",
              "        [6.1, 3. , 4.9, 1.8],\n",
              "        [6.4, 2.8, 5.6, 2.1],\n",
              "        [7.2, 3. , 5.8, 1.6],\n",
              "        [7.4, 2.8, 6.1, 1.9],\n",
              "        [7.9, 3.8, 6.4, 2. ],\n",
              "        [6.4, 2.8, 5.6, 2.2],\n",
              "        [6.3, 2.8, 5.1, 1.5],\n",
              "        [6.1, 2.6, 5.6, 1.4],\n",
              "        [7.7, 3. , 6.1, 2.3],\n",
              "        [6.3, 3.4, 5.6, 2.4],\n",
              "        [6.4, 3.1, 5.5, 1.8],\n",
              "        [6. , 3. , 4.8, 1.8],\n",
              "        [6.9, 3.1, 5.4, 2.1],\n",
              "        [6.7, 3.1, 5.6, 2.4],\n",
              "        [6.9, 3.1, 5.1, 2.3],\n",
              "        [5.8, 2.7, 5.1, 1.9],\n",
              "        [6.8, 3.2, 5.9, 2.3],\n",
              "        [6.7, 3.3, 5.7, 2.5],\n",
              "        [6.7, 3. , 5.2, 2.3],\n",
              "        [6.3, 2.5, 5. , 1.9],\n",
              "        [6.5, 3. , 5.2, 2. ],\n",
              "        [6.2, 3.4, 5.4, 2.3],\n",
              "        [5.9, 3. , 5.1, 1.8]]),\n",
              " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
              " 'frame': None,\n",
              " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
              " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
              " 'feature_names': ['sepal length (cm)',\n",
              "  'sepal width (cm)',\n",
              "  'petal length (cm)',\n",
              "  'petal width (cm)'],\n",
              " 'filename': 'iris.csv',\n",
              " 'data_module': 'sklearn.datasets.data'}"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CmsV6TQMtGw",
        "outputId": "a7c68661-14f6-4727-c378-6029cfda0af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = iris['data']"
      ],
      "metadata": {
        "id": "wdXS4hrJMtE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = iris['target']"
      ],
      "metadata": {
        "id": "srgmxBv5M-pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygdHSQ0pND-f",
        "outputId": "960cfbf6-bc04-453e-ddf5-b06b4fb7e084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nibUM5kRND7Y",
        "outputId": "bfb5c96e-02fe-44e3-bf3c-58d215b758de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('int64')"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "5uSsw5-JNMEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
      ],
      "metadata": {
        "id": "06ByZ_nfNP8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature columns\n",
        "feature_columns = [tf.feature_column.numeric_column(key=str(i)) for i in range(X.shape[1])]\n"
      ],
      "metadata": {
        "id": "_5n8zJVgnh8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DNNClassifier\n",
        "classifier = tf.estimator.DNNClassifier(\n",
        "    feature_columns=feature_columns,\n",
        "    hidden_units=[10, 20, 10],\n",
        "    n_classes=3\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfOmqFgad97r",
        "outputId": "3ef677c6-23cb-48ec-9152-8b1647e31c87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp4q395n4c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training input function\n",
        "train_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
        "    x={str(i): X_train[:, i] for i in range(X.shape[1])},\n",
        "    y=y_train,\n",
        "    batch_size=20,\n",
        "    num_epochs=None,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "-YCVtT2zoWa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "classifier.train(input_fn=train_input_fn, steps=2000) # Use .train() instead of .fit() for TensorFlow Estimators"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3phOat1nvP_",
        "outputId": "2de39ffa-1613-4255-8b4f-bd3527f838e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_estimator.python.estimator.canned.dnn.DNNClassifierV2 at 0x7f37f69e9d20>"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input function for evaluation\n",
        "def eval_input_fn():\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(({str(i): X_test[:, i] for i in range(X_test.shape[1])}, y_test))\n",
        "    dataset = dataset.batch(32)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "o3iPqAH0kxie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the generator to a list\n",
        "iris_predictions = list(classifier.predict(input_fn=eval_input_fn))"
      ],
      "metadata": {
        "id": "veZvII3Nk3kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the predicted class labels (assuming they are in a key called 'class_ids')\n",
        "iris_predictions = [pred['class_ids'][0] for pred in iris_predictions]"
      ],
      "metadata": {
        "id": "Se80IqlWk7c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "classifier.train(input_fn=train_input_fn, steps=2000) # Use .train() instead of .fit() for TensorFlow Estimators"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ22biSMeC9C",
        "outputId": "e8167c9d-efef-4fcb-841f-3094b6c8bd3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_estimator.python.estimator.canned.dnn.DNNClassifierV2 at 0x7f37f69e9d20>"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "print(confusion_matrix(y_test, iris_predictions))\n",
        "print(classification_report(y_test, iris_predictions))\n",
        "print(accuracy_score(y_test, iris_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmf9CYSJhSxu",
        "outputId": "98742a14-aeb1-41db-f857-5b0a8adc73aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[11  0  0]\n",
            " [ 0  0 17]\n",
            " [ 0  0 17]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        11\n",
            "           1       0.00      0.00      0.00        17\n",
            "           2       0.50      1.00      0.67        17\n",
            "\n",
            "    accuracy                           0.62        45\n",
            "   macro avg       0.50      0.67      0.56        45\n",
            "weighted avg       0.43      0.62      0.50        45\n",
            "\n",
            "0.6222222222222222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature columns\n",
        "feature_columns = [tf.feature_column.numeric_column(key=str(i)) for i in range(X.shape[1])]\n",
        "\n",
        "# DNNClassifier\n",
        "classifier = tf.estimator.DNNClassifier(\n",
        "    feature_columns=feature_columns,\n",
        "    hidden_units=[10, 20, 10],\n",
        "    n_classes=3\n",
        ")\n",
        "\n",
        "# Training input function\n",
        "train_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
        "    x={str(i): X_train[:, i] for i in range(X.shape[1])},\n",
        "    y=y_train,\n",
        "    batch_size=20,\n",
        "    num_epochs=None,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Train the classifier\n",
        "classifier.train(input_fn=train_input_fn, steps=2000)\n",
        "\n",
        "# Prediction input function\n",
        "test_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
        "    x={str(i): X_test[:, i] for i in range(X.shape[1])},\n",
        "    num_epochs=1,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Predict\n",
        "iris_predictions = list(classifier.predict(input_fn=test_input_fn))\n",
        "predicted_classes = [p['class_ids'][0] for p in iris_predictions]\n",
        "\n",
        "# Evaluation\n",
        "print(confusion_matrix(y_test, predicted_classes))\n",
        "print(classification_report(y_test, predicted_classes))\n",
        "print(accuracy_score(y_test, predicted_classes))\n"
      ],
      "metadata": {
        "id": "R8xU5MbqQBor",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "841316c0-da68-4a05-cd61-6cae61393f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmptksn65db\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0 11  0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       0.45      1.00      0.62         9\n",
            "           2       0.00      0.00      0.00        11\n",
            "\n",
            "    accuracy                           0.63        30\n",
            "   macro avg       0.48      0.67      0.54        30\n",
            "weighted avg       0.47      0.63      0.52        30\n",
            "\n",
            "0.6333333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "thjHRcpho0wI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define feature columns\n",
        "feature_columns = [\n",
        "    tf.feature_column.numeric_column('sepal_length'),\n",
        "    tf.feature_column.numeric_column('sepal_width'),\n",
        "    tf.feature_column.numeric_column('petal_length'),\n",
        "    tf.feature_column.numeric_column('petal_width')\n",
        "]\n",
        "\n",
        "# Define the input function\n",
        "def train_input_fn():\n",
        "    # Create a dataset from the features and labels\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {'sepal_length': X_train[:, 0],\n",
        "         'sepal_width': X_train[:, 1],\n",
        "         'petal_length': X_train[:, 2],\n",
        "         'petal_width': X_train[:, 3]},\n",
        "        y_train\n",
        "    ))\n",
        "    # Shuffle and batch the data\n",
        "    dataset = dataset.shuffle(1000).batch(32)\n",
        "    return dataset\n",
        "\n",
        "# Create the DNNClassifier\n",
        "classifier = tf.estimator.DNNClassifier(\n",
        "    feature_columns=feature_columns,\n",
        "    hidden_units=[10, 20, 10],\n",
        "    n_classes=3\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "classifier.train(input_fn=train_input_fn, steps=2000)\n",
        "\n",
        "# Define the input function for evaluation\n",
        "def eval_input_fn():\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {'sepal_length': X_test[:, 0],\n",
        "         'sepal_width': X_test[:, 1],\n",
        "         'petal_length': X_test[:, 2],\n",
        "         'petal_width': X_test[:, 3]},\n",
        "        y_test\n",
        "    ))\n",
        "    dataset = dataset.batch(32)\n",
        "    return dataset\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "eval_result = classifier.evaluate(input_fn=eval_input_fn)\n",
        "print(f\"Test set accuracy: {eval_result['accuracy']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfKfY8q5pevC",
        "outputId": "67113525-8727-4aa9-b745-726d036196d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmph3j5lslo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set accuracy: 0.2889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "def train_input_fn():\n",
        "    # Create a StandardScaler object\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the training data and transform it\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "    # Create a TensorFlow Dataset from the scaled training data\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X_train_scaled, y_train))\n",
        "\n",
        "    # Shuffle, batch, and repeat the dataset\n",
        "    dataset = dataset.shuffle(1000).batch(32).repeat()\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Convert the generator to a list\n",
        "iris_predictions = list(classifier.predict(input_fn=eval_input_fn))\n",
        "\n",
        "# Extract the predicted class labels (assuming they are in a key called 'class_ids')\n",
        "iris_predictions = [pred['class_ids'][0] for pred in iris_predictions]\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "print(confusion_matrix(y_test, iris_predictions))\n",
        "print(classification_report(y_test, iris_predictions))\n",
        "print(accuracy_score(y_test, iris_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maYZKulsqws9",
        "outputId": "274bb4a6-eeea-4c7a-c51d-6f7ec9592d7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0 16  0]\n",
            " [ 0 13  0]\n",
            " [ 0 16  0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        16\n",
            "           1       0.29      1.00      0.45        13\n",
            "           2       0.00      0.00      0.00        16\n",
            "\n",
            "    accuracy                           0.29        45\n",
            "   macro avg       0.10      0.33      0.15        45\n",
            "weighted avg       0.08      0.29      0.13        45\n",
            "\n",
            "0.28888888888888886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorflow 1.0x\n",
        "\n",
        " import tensorflow.contrib.learn.python.learn as learn\n",
        "\n",
        " classifier = learn.DNNClassifier(hidden_units=[10, 20, 10], n_classes=3)\n",
        " classifier.fit(X_train, y_train, steps=2000, batch_size=20)\n",
        "\n",
        "iris_predictions = classifier.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "print(confusion_matrix(y_test, iris_predictions))\n",
        "print(classification_report(y_test, iris_predictions))\n",
        "print(accuracy_score(y_test, iris_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "FpnyMXCgookn",
        "outputId": "d63598d8-3969-41e7-a79a-d6e6be41fc4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-109-d94579058b68>, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-109-d94579058b68>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    import tensorflow.contrib.learn.python.learn as learn\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VLtagifaq2AP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}