# -*- coding: utf-8 -*-
"""Comparison of Deep Learning and RandomForest Models for Bank Note Classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tc3dVjpSRlESsCe6hrfIJCnDTwu_s9lg
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

data = pd.read_csv('bank_note_data.csv')

data.head()

# @title Image.Var

from matplotlib import pyplot as plt
data['Image.Var'].plot(kind='hist', bins=20, title='Image.Var')
plt.gca().spines[['top', 'right',]].set_visible(False)

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
# %matplotlib inline

sns.countplot(x='Class', data=data)

sns.heatmap(data.corr())

sns.pairplot(data, hue='Class')

"""# **DATA PREPARATION**"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

scaler.fit(data.drop('Class', axis=1))

scaled_features = scaler.transform(data.drop('Class', axis=1))

df_feat = pd.DataFrame(scaled_features, columns=data.columns[:-1])
df_feat.head()

"""# **TRAIN-TEST SPLIT**"""

X = df_feat
y = data['Class']

X = X.values
y = y.values

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)

import tensorflow as tf # Import the TensorFlow library

# Define your feature columns
feature_columns = [tf.feature_column.numeric_column(key=str(i)) for i in range(X_train.shape[1])]

# Create the estimator
estimator = tf.estimator.DNNClassifier(hidden_units=[10, 20, 10], feature_columns=feature_columns, n_classes=2)

# Define input functions
train_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(
    x={str(i): X_train[:, i] for i in range(X_train.shape[1])},  # Create a dictionary for features
    y=y_train,
    num_epochs=None,
    shuffle=True
)

# def train_input_fn(features, labels, batch_size):
#    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))
#    dataset = dataset.shuffle(buffer_size=len(features))
#    dataset = dataset.batch(batch_size)
#    return dataset

# Train the estimator
estimator.train(input_fn=train_input_fn, steps=200)

"""# **Model Evaluation**"""

# Make predictions using the trained estimator
note_predictions = list(estimator.predict(input_fn=tf.compat.v1.estimator.inputs.numpy_input_fn(
    x={str(i): X_test[:, i] for i in range(X_test.shape[1])},
    shuffle=False)))

import numpy as np

from sklearn.metrics import classification_report, confusion_matrix

# Extract the predicted labels from the list of dictionaries
predicted_labels = [np.argmax(pred['logits']) for pred in note_predictions]

print(confusion_matrix(y_test, predicted_labels))

print(classification_report(y_test, predicted_labels))

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
data = pd.read_csv('bank_note_data.csv')
data.head()

# Plotting Image.Var
from matplotlib import pyplot as plt
data['Image.Var'].plot(kind='hist', bins=20, title='Image.Var')
plt.gca().spines[['top', 'right']].set_visible(False)
plt.show()

import seaborn as sns
# %matplotlib inline

sns.countplot(x='Class', data=data)
plt.show()

sns.heatmap(data.corr(), annot=True, cmap='coolwarm')
plt.show()

sns.pairplot(data, hue='Class')
plt.show()

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(data.drop('Class', axis=1))

scaled_features = scaler.transform(data.drop('Class', axis=1))

df_feat = pd.DataFrame(scaled_features, columns=data.columns[:-1])
df_feat.head()

X = df_feat
y = data['Class']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Create a Sequential model
model = Sequential()

# Add layers to the model
model.add(Dense(10, input_shape=(X_train.shape[1],), activation='relu'))
model.add(Dense(20, activation='relu'))
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=200, batch_size=20, verbose=1)

# Make predictions
note_predictions = (model.predict(X_test) > 0.5).astype("int32")

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, note_predictions))
print(classification_report(y_test, note_predictions))

"""# **COMPARISON**"""

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(n_estimators=200)

rfc.fit(X_train, y_train)

rfc.score(X_test, y_test)

rfc_pred = rfc.predict(X_test)

print(confusion_matrix(y_test, rfc_pred))

print (classification_report(y_test, rfc_pred))

# Commented out IPython magic to ensure Python compatibility.
# Tensorflow 1.X


from google.colab import files
uploaded = files.upload()

import pandas as pd
data = pd.read_csv('bank_note_data.csv')
data.head()

# @title Image.Var

from matplotlib import pyplot as plt
data['Image.Var'].plot(kind='hist', bins=20, title='Image.Var')
plt.gca().spines[['top', 'right',]].set_visible(False)

import seaborn as sns
# %matplotlib inline

sns.countplot(x='Class', data=data)

sns.heatmap(data.corr())

sns.pairplot(data, hue='Class')

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

scaler.fit(data.drop('Class', axis=1))

scaled_features = scaler.transform(data.drop('Class', axis=1))

df_feat = pd.DataFrame(scaled_features, columns=data.columns[:-1])
df_feat.head()

X = df_feat
y = data['Class']

X = X.values
y = y.values

from sklearn.model_selection import train_test

import tensorflow as tf # Import the TensorFlow library

import tensorflow.contrib.learn.python.learn as learn
classifier = learn.DNNClassifier(hidden_units=[10, 20, 10], n_classes=2)
classifier.fit(X_train, y_train, steps=200, batch_size=20)

note_predictions = classifier.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, note_predictions))
print(classification_report(y_test, note_predictions))

"""# **Conclusion**
Both models performed extremely well on the dataset. The Deep Learning model achieved perfect performance across all metrics, indicating it was able to perfectly distinguish between the classes. The RandomForest model also performed excellently, with only slight differences in precision and recall for one of the classes. Given the near-perfect performance of both models, the choice between them might come down to other factors such as computational resources, interpretability, and training time.
"""