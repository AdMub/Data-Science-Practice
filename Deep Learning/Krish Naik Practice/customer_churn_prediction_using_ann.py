# -*- coding: utf-8 -*-
"""Customer Churn Prediction Using ANN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15oCwZhDADQOHuS9Zu-ryK3LAu4Me9TxZ
"""

!pip install --upgrade pip setuptools

!pip install tensorflow

!pip install tensorflow==2.11.0

import tensorflow as tf
print(tf.__version__)

"""# **Load and Display the Data**"""

## import some basic libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

data = pd.read_csv('Churn_Modelling.csv')
data.head()

"""The dataset has been successfully loaded. It includes various features such as CreditScore, Geography, Gender, Age, Tenure, Balance, NumOfProducts, HasCrCard, IsActiveMember, EstimatedSalary, and Exited.

# **Divide the Dataset into Independent and Dependent Features**

The dataset is a Binary Classificaton problem, we need to predict whether the customers will exit that particular banks or any company

Exited column - Independent Features

Remaining column(except RowNumber,CustomerId, Surname)  - Dependent Features

Firstly, We need to convert the entire datasets to Independent and Dependent features
"""

## Divide the dataset into Independent and Dependent features
## X = Independent
## y = Dependent

X = data.iloc[:, 3:-1]
y = data.iloc[:, -1]

## OR
X = data.iloc[:, 3:13]
y = data.iloc[:, 13]

X.head()

y.head()

"""The dataset in the Independent features are not clean because they have categorical features(i.e Geography, Gender), then it must fixed

# **Feature Engineering**
"""

## Feature Engineering
geography = pd.get_dummies(X["Geography"], drop_first=True)
gender = pd.get_dummies(X["Gender"], drop_first=True)

## Drop Geography and Gender from the dataframe
X = X.drop(["Geography", "Gender"], axis=1)
X.head()

## concatenate these variables with dataframe
X = pd.concat([X, geography, gender], axis=1)
X.head()

"""# **Split the Dataset into Training and Test Sets**"""

# Splitting the dataset into Training set and Test Set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

"""# **Feature Scaling**"""

## Feature Scaling

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

X_train

X_test

X_train.shape

"""# **Create and Train the ANN**"""

## Now let's create the ANN
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LeakyReLU, PReLU, ELU, ReLU
from tensorflow.keras.layers import Dropout

## Initialize the ANN
classifier = Sequential()

## Adding the input layer
classifier.add(Dense(units=11, activation='relu'))
classifier.add(Dropout(0.2))

## Adding the first hidden layers
classifier.add(Dense(units=7, activation='relu'))
classifier.add(Dropout(0.3))

## Adding the second hidden layers
classifier.add(Dense(units=6, activation='relu'))

## Adding  the output layer
classifier.add(Dense(units=1, activation='sigmoid'))

classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

"""To provide your Learning_Rate because initially adam optimizer have 0.01, but to provide your Learning_Rate"""

import tensorflow
opt = tensorflow.keras.optimizers.Adam(learning_rate=0.01)

classifier.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])

## Train my neuron network
model_history = classifier.fit(X_train, y_train, validation_split=0.33, batch_size=10, epochs=1000)

model_history.history.keys()

# summarize history for accuracy
plt.plot(model_history.history['accuracy'])
plt.plot(model_history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show

import matplotlib.pyplot as plt

# Assuming model_history is the history object returned by the model's fit method
plt.plot(model_history.history['loss'], label='Train Loss')
plt.plot(model_history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss Over Epochs')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""**Introduction of Early Stopping**

Early Stopping is when the accuracy is not increasing, it automatically stop the training model (i.e. Stop training when a monitored metric has stopped improving)
"""

## Early Stopping
import tensorflow as tf
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.0001,
    patience=20,
    verbose=1,
    mode="auto",
    baseline=None,
)

## Train my neuron network
model_history_1 = classifier.fit(X_train, y_train, validation_split=0.33, batch_size=10, epochs=1000, callbacks=early_stopping)

model_history_1.history.keys()

# summarize history for accuracy
plt.plot(model_history_1.history['accuracy'])
plt.plot(model_history_1.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(model_history_1.history['loss'])
plt.plot(model_history_1.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show

import matplotlib.pyplot as plt

# Assuming model_history_1 is the history object returned by the model's fit method
plt.plot(model_history_1.history['loss'], label='Train Loss')
plt.plot(model_history_1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss Over Epochs')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""# **Prediction and Evaluation of Model**"""

## Predicting  the Test results
y_pred = classifier.predict(X_test)
y_pred = (y_pred >= 0.5)

# Make the confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
cm

# Calculate the accuracy
from sklearn.metrics import accuracy_score
score = accuracy_score(y_pred, y_test)
score

# Calculate the classification report
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

# Get the weights
classifier.get_weights()

"""Here's the step-by-step process:

1.   Load and display the data.
2.   Divide the dataset into independent and dependent features.
3.   Feature engineering.
4.   Split the dataset into training and test sets.
5.   Feature scaling.
6.   Create and train the ANN.
7.   Plot training history.
8.   Apply early stopping.
7.   Predict and evaluate the model.

The required **libraries** for running the project are:

**Numpy**: For numerical computations.

**Pandas**: For data manipulation and analysis.

**Matplotlib**: For plotting graphs.

**Scikit-learn**: For preprocessing and splitting the dataset, as well as for metrics like confusion matrix and accuracy score.

**TensorFlow**: For building, training, and evaluating the neural network.

# **ANN-Based Customer Churn Prediction**

**Overview**

This repository contains a project focused on predicting customer churn using an Artificial Neural Network (ANN). Customer churn, or customer attrition, is when customers stop doing business with a company. Identifying and understanding the factors leading to churn is vital for businesses to retain customers and enhance overall satisfaction.

**Dataset**

The dataset used in this project is Churn_Modelling.csv, which includes various features related to customer demographics and their interactions with the business. Key features include Geography, Gender, Age, Tenure, Balance, and more. The objective is to build a model that accurately predicts whether a customer will churn.

**Project Steps**

**Data Cleaning and Preprocessing:**

*   Loaded the dataset and checked for missing values and duplicates.
*   Handled categorical variables using one-hot encoding for 'Geography' and 'Gender'.
*   Dropped the original 'Geography' and 'Gender' columns after encoding.

**Data Splitting:**

Divided the dataset into independent (X) and dependent (y) features.
Split the data into training and test sets (80-20 split).

**Feature Scaling:**

*   Standardized the features using StandardScaler to ensure they have a mean of 0 and a standard deviation of 1.


**Model Building:**

Designed and built an ANN using the Sequential model from TensorFlow's Keras.
Added input layer, hidden layers with dropout for regularization, and an output layer.
Used ReLU activation for hidden layers and sigmoid activation for the output layer.
Compiled the model with Adam optimizer and binary cross-entropy loss function.


**Model Training:**

Trained the model on the training set with a validation split and early stopping to prevent overfitting.


**Evaluation:**

Evaluated the model's performance using a confusion matrix, accuracy score, and classification report.


# **Results**

**Confusion Matrix:**

       array([[1451,  144],
       [ 172,  233]])


True Positives (TP): 233

True Negatives (TN): 1451

False Positives (FP): 144

False Negatives (FN): 172


Accuracy: 0.842 (84.2%)



**Classification Report:**



              precision    recall  f1-score   support

           0       0.89      0.91      0.90      1595
           1       0.62      0.58      0.60       405

    accuracy                           0.84      2000
   macro avg       0.76      0.74      0.75      2000
weighted avg       0.84      0.84      0.84      2000


# **Conclusion**

The ANN model developed in this project effectively predicts customer churn with an accuracy of 84.2%. The model demonstrates high precision and recall for non-churn customers, while there is room for improvement in identifying churn customers. The insights and methodology outlined in this project can be used to develop strategies aimed at reducing customer churn.

Feel free to explore the code and visualizations to gain a deeper understanding of the dataset and the modeling process.

Happy coding! 🚀
"""